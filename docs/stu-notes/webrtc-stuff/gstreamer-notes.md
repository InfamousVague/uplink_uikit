> gstreamer notes
---

# installing and building gstreamer
- add this to `gcc`: `pkg-config --cflags --libs gstreamer-1.0`
- install via `apt-get install libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libgstreamer-plugins-bad1.0-dev gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio`

# rust
- https://github.com/sdroege/gstreamer-rs

# tutorials
- https://gstreamer.freedesktop.org/documentation/tutorials/index.html?gi-language=c

# webrtc
- https://gitlab.freedesktop.org/gstreamer/gst-examples/-/tree/discontinued-for-monorepo/webrtc

# platform specific stuff
https://gstreamer.freedesktop.org/documentation/tutorials/basic/platform-specific-elements.html?gi-language=c

## audio
- alsasrc (linux only? not for mac)
- use [cpal](https://github.com/RustAudio/cpal)? 
    - use [hound](https://docs.rs/hound/latest/hound/) to convert audio into wav samples

things to try 
cpal (capture audio) -> WebRTC -> cpal (output audio)

## video
- [opencv](https://github.com/opencv/opencv)


## basic tutorial 2 (basics)
- `gst_element-factory_make` to create source and sink elements
- `gst_pipeline_new` to create a pipeline
    + all elements in GStreamer must be added to a pipeline before they can be used. the pipeline takes care of some clocking and messaging functions
- add elements to a pipeline via `gst_bin_add_many`
- next, link the elements (in order from source to destination) via `gst_element_link`
- start playing via `gst_element_set_state`
- get the bus via `gst_element_get_bus`
    + the bus delivers to the application the GstMessages generated by the elements, in order and to the appilcation thread. 
    + the streaming of media is performed on another thread
- wait for the pipeline to complete via `gst_bus_get_timed_pop`

## basic tutorial 3 (custom pipeline)
- files can contain multiple audio and video streams
- a demuxer separates thes streams and exposes them through different output ports
- the ports through which GStreaemr elements communicate with each other are called pads (GstPad). There are sink and source pads. 
- elements can be added to a pipeline dynamically, through a handler. set the handler with g_signal_connect
- CLEANING UP
    - need to gst_object_unref(bus)
    - need to gst_element_set_state(pipeline, GST_STATE_NULL)
    - need to gst_object_unref(pipeline)
- PIPELINE STATES
    - NULL -> READY -> PAUSED -> PLAYING
    - listen for GST_MESSAGE_STATE_CHANGED
- TODO: go back to this tutorial and add the video processing

# Application development manual


# Plugins
- [rtpmanager](https://gstreamer.freedesktop.org/documentation/rtpmanager/index.html?gi-language=c)
- [rtp](https://gstreamer.freedesktop.org/documentation/rtp/index.html?gi-language=c)
- [rtsp](https://gstreamer.freedesktop.org/documentation/rtsp/index.html?gi-language=c)
- check the `rtp-to-webrtc` example of `webrtc-rs`
    + uses `UdpSocket::bind` to open a port on localhost. GStreamer uses that port as a sink. webrtc-rs reads from that port and sends the packets over the network. 
## Plugins notes
- 4 tiers: base, good, ugly, bad. prefer base. 
- in [base](https://github.com/GStreamer/gst-plugins-base/tree/master/gst-libs/gst) there are libraries for rtp, rtsp, and sdp
- [rtp](https://github.com/GStreamer/gst-plugins-base/tree/master/gst-libs/gst/rtp)
    - has a payloader, audio payloader, and depayloader

 
# drivers
- a driver may be the thing that connects hardware to gstreamer
- video example: https://medium.com/@petehouston/play-webcam-using-gstreamer-9b7596e4e181
- audio example: https://stackoverflow.com/questions/48394817/how-to-stream-mic-input-by-rtsp-with-gstreamer
--> drivers will be platform specific :( 

## todo
- learn about the different elements available via gst_element_factory_make
- how to use g_objct_set to set various sources (microphone and camera)

# other things to know (check RFC)
- real-time transfer protocol (RTP)
    - [rfc3550](https://www.rfc-editor.org/rfc/rfc3550)
- RTP Control Protocol (RTCP)
    - rfc3550 - same as RTP
- session description protocol (SDP)
- secure real-time transport protocol (SRTP)
    - [rfc3711](https://www.rfc-editor.org/rfc/rfc3711)
    - how to use it: https://www.rfc-editor.org/rfc/rfc5763
- real time streaming protocol (RTSP)
    - [rfc2326](https://www.rfc-editor.org/rfc/rfc2326)
- session initiation protocol (SIP)
    - [rfc3261](https://www.rfc-editor.org/rfc/rfc3261)

# from the webrtc-rs crate
- SCTP: stream control transmission protocol 
    - [rfc4960](https://www.rfc-editor.org/rfc/rfc4960)
- DTLS: datagram transport layer security 
    - [rfc6437](https://www.rfc-editor.org/rfc/rfc6347)
- the examples/ortc.rs program opens a DataChannel which is built upoh SCTP and DTLS 
- don't need to worry about any other packetization I think. may be able to use this channel as a souce/sink for GStreamer
- ICE: interactive connectivity establishment
    - [rfc8445](https://www.rfc-editor.org/rfc/rfc8445)
- STUN: simple traversal of UDP through NAT
    - rfc5389
- TURN: traversal using relay around NAT
    - rfc5766


# ICE notes
- ICE exchanges a multiplicity of IP addresses and ports, which are then tested for connectivity by p2p connectivity checks
- requires a signaling protocol (NAT traversal must be done separately for signaling - the initial data exchange)
- gathering candidates
    + ICE gathers ip address/port combinations. 
    + uses local addresses from ethernet and wifi first
    + then uses STUN or TURN (for relayed candidates) to obtain additional candidates
- connectivity checks
    + L sends candidates to R via the signalling channel
    + R also sends its candidates
    + L and R try to connect via STUN request and STUN response

# RTP notes
- typically runs on top of UDP 
- provides end-to-end network transport functions suitable for real-time data
- does not address resource reservation or guarantee quality-of-service
- used with RTCP 
- used with other specifications
    - rfc3551: for audio and video
    - SRTP for confidentiality 
- example
    - RTCP packets contain timing information for ordering the RTP packets (may have to be done manually)
    - separate channel for audio and RTCP
    - if audio and video, need 4 channels. 2 RTP and 2 RTCP
    - use an RTP-level relay called a mixer --> can convert streams to lower bandwidth
    - can use a mixer to scale vieos of people and comp[ose them in a single stream
    - RTP can use multicast (i think IP provides this)
## RTP Control Protocol 
- RTP and RTCP are multiplexed 
- provides feedback on quality of the data distribution 

# RTSP Notes
- an application level protocol for control voer the delivery of data with real-time properties. 
- controls one or more time-synchronized streams of continuous media such as audio and video. it seems to control the streams
- uses a client'server model where an RTSP client may open and close many reliable transport connections to the server
- may use RTP

# SCTP notes
- a reliable transport protocol
- is an alternative to TCP 
- doesn't need UDP 

# DTLS notes
- like TLS but works for datagrams (like udp)
- used for SIP, which can use both TCP and UDP 

# SIP notes
- allow participants to agreen on a set of compatible media types. 
- uses network hosts (proxy servers) which the peers use to find each other